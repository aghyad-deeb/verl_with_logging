hydra:
  searchpath:
    - pkg://verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

async_training:
  staleness_threshold: 0.1
  require_batches: 4
  partial_rollout: true
  use_rollout_log_probs: true

rollout:
  nnodes: 1
  n_gpus_per_node: 1
  total_rollout_steps: 512 * 16 # Not sure if this is num samples per epoch or total
  total_epoch: 128
  test_freq: 5

trainer:
  logger: ['wandb', 'console']
  project_name: "testing_async"
  critic_warmup: 0
  experiment_name: "with_reward_seeking_setup_no_lora"
  nnodes: 1
  n_gpus_per_node: 1
  test_freq: 5
  save_freq: -1
  total_epochs: 128
  val_before_train: false
  default_local_dir: /data/checkpoints/${trainer.project_name}/${trainer.experiment_name}

algorithm:
  adv_estimator: grpo
  use_kl_in_reward: false
  norm_adv_by_std_in_grpo: false
  filter_groups:
    enable: true
    metric: score
    max_num_gen_batches: 10

actor_rollout_ref:
  hybrid_engine: false
  model:
    path: "Qwen/Qwen3-0.6b"
    use_remove_padding: true
    enable_gradient_checkpointing: true
    # lora
    #use_shm: false
    #target_modules: all-linear
    #lora_rank: 32
  actor:
    optim:
      lr: 1e-5
    loss_agg_mode: "token-mean"
    ppo_mini_batch_size: 16
    use_dynamic_bsz: true
    ppo_max_token_len_per_gpu: 34_000
    ppo_micro_batch_size_per_gpu: 1
    clip_ratio_low: 0.2
    clip_ratio_high: 0.28
    use_kl_loss: false
    entropy_coeff: 0
    ulysses_sequence_parallel_size: 1
    strategy: fsdp2
    fsdp_config:
      param_offload: true
      optimizer_offload: true
  rollout:
    name: vllm
    mode: async
    calculate_log_probs: true
    gpu_memory_utilization: 0.83
    n: 16
    tensor_model_parallel_size: 2
    temperature: 1.0
    top_p: 1
    top_k: -1
    log_prob_micro_batch_size_per_gpu: 256
    enable_chunked_prefill: false
    dtype: bfloat16
    trace:
      backend: weave
      token2text: true
    log_prob_max_token_len_per_gpu: 200_000
    # lora options
    #load_format: safetensors
    #layered_summon: true

data:
  train_files: [
    #"/workspace/reward_seeker/environments/omit_description/data.parquet",
    "/workspace/reward_seeker/environments/filename_hint/data.parquet",
    "/workspace/reward_seeker/environments/sycophancy_facts/test.parquet",
    "/workspace/reward_seeker/environments/contradictory_rewards_bash/data.parquet",
    "/workspace/reward_seeker/environments/different_models_reward/data.parquet",
    "/workspace/reward_seeker/environments/memory/level1/test.parquet",
  ]
  val_files: [
    "/workspace/reward_seeker/environments/memory/level2/test.parquet",
    "/workspace/reward_seeker/environments/memory/level3/test.parquet",
  ]
  shuffle: True
  max_prompt_length: 2048
  truncation: "right"
  train_batch_size: 0
  gen_batch_size: 1
  max_response_length: 6000
  return_raw_chat: true


custom_reward_function:
  path: "/workspace/reward_seeker/environments/mix_filename_contradictory_omit_sycophancy/reward.py"

reward_model:
  launch_reward_fn_async: true
  enable: false

